---
title: "PH125_9 Choose Your Own Capstone Project"
author: "Mario De Toma"
date: "May 26, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

## Objectives
*Motivation)*
This project has been conducted as part of the Data Science Professional Certification path provided by  HarvardX, an online learning initiative of Harvard University through edX.
In particular this is the second data science project to submit for PH125.9x Course denominated "Data Science: Capstone".
The name of the project, Choose Your Own, is due to the fact that the dataset under analysis could be chosen by the learner from public available datasets.

I chose the Fall Detection dataset from the curated list of datasets at the following link https://www.kaggle.com/annavictoria/ml-friendly-public-datasets?utm_medium=email&utm_source=intercom&utm_campaign=data+projects+onboarding , a repository indicated by Course Staff. 

*Project objective)*
As per course project introduction the project aim is to apply machine learning techniques that go beyond standard linear regression. 
In particular the task of this project is  multi class classification i.e. where the outcome variable is categorical with more than 2 classes. 
Specifically the problem statement is related to predict the type of activity among 6 different activities of daily living (ADLs) on the basis of monitored medical measures obtained through sensors worn by elder people as recorded in the dataset analysed.


*Research question)*
The research question can be stated as:
is it possible to predict 6 activities of daily living (ADLs) including Standing , Walking, Sitting, Falling,  Cramps, Running from few predictors monitoring health status?

This project is not intended to solve the problem of "Fall Detection", as the name of the dataset could suggest, which could have been settled by binarysing the ADL information as Falling vs all the remaining activities.

Furthermore note that no causal inference claim can be raised after this study that focus only on supervised learning.

*Dataset)*
The Fall detection dataset of Chinese hospitals of old age patients [1] is hosted by kaggle. 
It reports 16382 observations containing the ADL label and related 6 numerical predictors measuring the medical characteristics monitored.

## Background
Starting point for conducting this study was the supervised leaning process as decribed by Professor Rafael Irizarry in the PH125_8 edX course on Machine Learning and in his book: Introduction to Data Science [2].
In particular this study was conducted using the R package 'caret' as the framework for machine learning [3].

## Overview and outline
The study demonstrates that ADLs can be predicted and that the reachable accuracy of prediction depends on the model chosen.

The rest of this report is articulated in the following sections:

- *Methods*: in this section the dataset is explored in order to find useful insight, then the design of the study is explained and different models are proposed. Finally the modeling is described in details.

- *Results*: this section shows actual results achieved and compares diverse models evaluation. Furthermore, in order to explain the local behavior of the best model, the ceteris paribus analysis procedure is shown

- *Conclusions*: the section summarizes achievement, it discusses the project validity and it indicates potential  model improvements

- *Reproducibility*: this section provides information related to the reproducibility of the analysis including computation considerations, HW and SW stack used.

# Methods
In order to answer the research question posed in the introcuction section, the dataset Fall Detection has been analyzed and then the machine learning experiments conducted using different machine learning tecniques adequate for the multi class classificaion task.

```{r libraries, include=FALSE }
if(!require(tidyverse)) {
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
  library(tidyverse)
}
if(!require(caret)) {
  install.packages("caret", repos = "http://cran.us.r-project.org")
  library(caret)
}
```

## Exploratory Data Analysis
Fall detection dataset has been downloaded from kaggle, put on my github and then loaded into R and partitioned such that 70% of the observations belong to the training set (fall_train) and 30% the test set (fall_test). The caret::createDataPartition function has been used in order to mantain the class distribution between train and test set so that the main assumption of every prediction study is respected:
train, test and future "production" data all come from the same data generating process producing the same distribution.
```{r load data, message=FALSE, include=FALSE}
data_file_path <- 'https://raw.githubusercontent.com/mdt-ds/PH125.9_cyo_fallDetection/master/falldeteciton.csv'
fall_detection <- read_csv(file = data_file_path)
set.seed(525)
train_idx <- createDataPartition(y = fall_detection$ACTIVITY, times = 1, 
                                 p = 0.7, list = FALSE)
fall_train <- fall_detection %>% slice(train_idx)
fall_test <- fall_detection %>% slice(-train_idx)
rm(data_file_path, train_idx)
```

Fall Detection dataset contains the following valriables:
```{r Fall Detection structure, echo=FALSE}
glimpse(fall_train)
```
The outcome variable ACTIVITY is numeric. In order to enable classification it has been converted to a factor with the following levels: Standing, Walking, Sitting, Falling, Cramps, Running.
```{r labels, include=FALSE}
fall_train <- fall_train %>% mutate(ACTIVITY = factor(ACTIVITY, 
                             labels = c('Standing', 'Walking', 'Sitting', 
                                        'Falling', 'Cramps', 'Running')))
fall_test <- fall_test %>% mutate(ACTIVITY = factor(ACTIVITY, 
                              labels = c('Standing', 'Walking', 'Sitting', 
                                        'Falling', 'Cramps', 'Running')))

```

### ADL Distribution
The ADL classes are not evenly distributed. In particular Walking ADL has few observations. This could make our multi class classification task harder.

```{r ADLs Distribution, echo=FALSE}
ggplot(fall_train, aes(ACTIVITY)) +
  geom_bar(aes(fill = ACTIVITY)) +
  labs(title = 'ADLs Distribution')
```

### Predictors discriminative power
Predictors for ADL in Fall Detection dataset are all numeric and they specifically are:

- TIME monitoring time

- SL sugar level

- EEG monitoring rate

- BP blood pressure

- HR Heart beat rate

- CIRCLUATION Blood circulation (the typo in the original dataset has been kept)

In order to check single predictor capacity to discriminate among classes, the boxplot by class for each predictor has been drawn.

```{r boxplot by ADL, echo=FALSE}
timeBP <- ggplot(fall_train, aes(ACTIVITY, TIME)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('monitoring time') +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 7))

slBP <- ggplot(fall_train, aes(ACTIVITY, SL)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('sugar level') +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 7))

eegBP <- ggplot(fall_train, aes(ACTIVITY, EEG)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('EEG monitoring rate') +
  theme(legend.position = 'none',
        axis.text.x = element_text(size = 7))

bloodBP <- ggplot(fall_train, aes(ACTIVITY, BP)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('blood pressure') +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 7))

hrBP <- ggplot(fall_train, aes(ACTIVITY, HR)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('heart beat rate') +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 7))

circBP <- ggplot(fall_train, aes(ACTIVITY, CIRCLUATION)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('blood circulation') +
  theme(legend.position = 'none',
        axis.text.x = element_text(size = 7))

if(!require(gridExtra)) {
  install.packages("gridExtra", repos = "http://cran.us.r-project.org")
  library(gridExtra)
}
grid.arrange(timeBP, slBP, bloodBP, hrBP, circBP, eegBP, 
              nrow = 3, ncol = 2, newpage = TRUE)

```
Considering the predictor by class distributions overlapping, the discriminative power of single predictors does not seem to be enough to classify ADL wiht a good accuracy. 

### Predictors correlation
Further element of analysis is related to the high level of correlation among predictors.

```{r corr, echo=FALSE}
if(!require(GGally)) {
  install.packages("GGally", repos = "http://cran.us.r-project.org")
  library(GGally)
}
ggpairs(fall_train %>% select(-ACTIVITY), axisLabels = 'none')
```
In particular SL (sugar level) and CIRCLUATION (blood circulation) with a correlation of 0.977 and TIME (monitoring time) and HR (heart beat rate) with a correlation of 0.974 are almost colinear. Also HR and CIRCULATION (0.904), HR with SL (0.857) and TIME with CIRCLUATION (0.877) correlations are really high.

### Feature importance
The feature importance has been therefore investigated through the random forest algorithm which provide as a side outcome the importance of a feature in discriminating one class from the other. In other words, for random forest algorithm it is easy to compute how much each variable is contributing to the classification decision.

```{r varImp, echo=FALSE}
if(!require(randomForest)) {
  install.packages("randomForest", repos = "http://cran.us.r-project.org")
  library(randomForest)
}

rf_mdl <- randomForest(ACTIVITY ~ ., data = fall_train)
varImpPlot(rf_mdl)
```
From the plot, SL results the most important predictor followed by EEG but all 6 predictors are of great help in the classification attempt since each contributes to decrease the GINI index. This index measures the decrease in node impurities from splitting on the variable, averaged over all trees.

## Proposed models
In order to accomplish the multi class classification task the following models have been tried:

- multinomial

- k Nearest Neighbors

- random forest

All the models are natural choices for multi class classification.

## Study design
The study has been conducted using the training set, fall_train, for training the model and tuning hyperparameters through cross validation while the final accuracy has been evaluated on the test set, fall_test.

The test dataset has not been used in any former phase of the study and therefore it could simulate new data and it allowed to evaluate the capacity of the model to generalize .

Accuracy metric has been used to evaluate and compare different models. For each model the confusion matrix has been produced in order to evaluate which class is harder to identify.

Cross validation has been configured in caret machine learning framework with 5 folds (80% for training, 20% for validation).

```{r train control}
ctrl <- trainControl(method = 'cv', number = 5, p = .8)
```

Cross validation lead to long computation times because in the defined study design the model has to be trained and validated 5 times.
Therefore cross validation computation has been parallelized making use of doParallel package [4] and of the multithread architecture of the HW used for this project.
```{r parallel, message=FALSE, warning=FALSE, include=FALSE}
if(!require(doParallel)) {
  install.packages("doParallel", repos = "http://cran.us.r-project.org")
  library(doParallel)
}
```


## Multinomial Logistic Regression
Multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems. The probability to belong to a particular class given the predictors is formulated as:

$$P(y=k|x^{(i)},\theta)=\frac{\exp(\theta^{(k)T}x^{(i)})}{\sum_{j=1}^k\exp(\theta^{(j)T}x^{(i)})}$$

where k is the class to be predicted, $x^{(i)}$ the features vector for observation $^{(i)}$ and $\theta$ the coefficient vector learned by the learner algorithm for each class.

It is also called softmax regression.

The multinomial model is implemented in R via neural networks by 'nnet' package [5].
Under the 'caret' framework it is possible to tune the following parameter:
```{r multinomParam, echo=FALSE}
modelLookup(model = 'multinom')
```
The tuning parameter Weight Decay is specific to neural networks and it helps the optimization process avoiding over-fitting.
The training process has been performed in two steps, the first went through a wide range of values for the hyperparameter
```{r multinom rough tune}
tunegrid <- data.frame(decay = seq(from = 0, to = 100, by = 10))
```


```{r multinomial rough train, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
if(!require(nnet)) {
  install.packages("nnet", repos = "http://cran.us.r-project.org")
  library(nnet)
}

# setup parallel computing
cl <- makePSOCKcluster(parallel::detectCores()-1)
registerDoParallel(cl)

# train model
set.seed(525) 
multinom_mdl <- train(ACTIVITY ~ .,
                 data = fall_train,
                 method = "multinom", 
                 tuneGrid = tunegrid, 
                 trControl = ctrl)

#Shutdown cluster
stopCluster(cl)


```
The resulting cross-validation plot identify the range where the best model can be found.

```{r multinom rough plot, echo=FALSE}
plot(multinom_mdl)
```

Then the training effort has been restricted to the following range:

```{r multinom final tune}
tunegrid <- data.frame(decay = seq(from = 0, to = 20, by = 1))
```

```{r multinom final train, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}

# setup parallel computing
cl <- makePSOCKcluster(parallel::detectCores()-1)
registerDoParallel(cl)

# train model
set.seed(525) 
multinom_mdl <- train(ACTIVITY ~ .,
                 data = fall_train,
                 method = "multinom", 
                 tuneGrid = tunegrid, 
                 trControl = ctrl)

#Shutdown cluster
stopCluster(cl)

```

The resulting cross-validation plot identify the best model.

```{r multinom final plot, echo=FALSE}
plot(multinom_mdl)
```

After cross-validation training through the defined tune grid the best model found has the following tuning parameter:

```{r multinom tuning, echo=FALSE}
multinom_mdl$bestTune
```

After evaluating the accuracy of the model prediction to the test unseen data, the confusion matrix and the accuracy overall score is displayed. 
```{r multinom evaluation, echo=FALSE}
y_hat <- predict(multinom_mdl, newdata = fall_test, type = 'raw')
confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$table
test_accuracy_multinom <- confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$overall['Accuracy']
paste('model test accuracy:', round(test_accuracy_multinom, 5), sep = '   ')
```
As per this results, the multinomial model can be discarded.
In some sense the bad accuracy results were expected since predictors are correlated and interconnected while multinomial model, as a type of generalized linear model, does not take into account interactions if not expressely defined in the model.
Furtermore we can see from the confusion matrix that Walking and Running ADLs are never predicted probably because they have few occurrences in the dataset in respect of other ADLs.
```{r multinom test result, include=FALSE}
test_results <- tibble(method = 'multinom', accuracy = test_accuracy_multinom)

```


## k Nearest Neighbour
k Nearest Neighbors is a non-parametric classification method that make use of distance (or similarity) measures. In particular for numerical predictors the euclidean distance is used. Euclidean distance is the length of the segment connecting 2 data points in the predictor space and it is defined as:

$$d(\overrightarrow{x}^{(i)},\overrightarrow{x}^{(j)}) = \sqrt{(x_1^{(i)}-x_1^{(j)})^2+(x_2^{(i)}-x_2^{(j)})^2+...+(x_p^{(i)}-x_p^{(j)})^2}$$

The kNN algorithm stored all the data and classify new data points in relation of majority of k nearest (as per euclidean distance) points class.

The k Nearest Neighbors model is implemented in R by 'e1071' package [6].
Under the 'caret' machine learning famework it is possible to tune the following hyperparameter:
```{r knnParam, echo=FALSE}
modelLookup(model = 'knn')
```
As tuning parameter k, number of nearest neighbors to consider, increases the decision boundary gets more smooth. k can be thought as a mean of regularizaion.
The training process will go through the following value for hyperparameters
```{r knn tune}
tunegrid <- data.frame(k= seq(1, 30, 1))
```
Given that kNN algorithm is based on distance/similarity measures, data needs to be scaled (by dividing by respective standard deviation) and centered (by subctracing the mean) before traning  in order to avoid that predictors with largest numerical range mask the effect of other predictors. 
```{r scale data, echo=FALSE}
fall_train_norm <- predict(preProcess(fall_train, method = c("center", "scale")),
                            newdata = fall_train)
fall_test_norm <- predict(preProcess(fall_train, method = c("center", "scale")),
                          newdata = fall_test)
```


```{r knn, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
if(!require(e1071)) {
  install.packages("e1071", repos = "http://cran.us.r-project.org")
  library(e1071)
}

# setup parallel computing
cl <- makePSOCKcluster(parallel::detectCores()-1)
registerDoParallel(cl)

# train model
set.seed(525) 
knn_mdl <- train(ACTIVITY ~ .,
                 data = fall_train_norm,
                 method = "knn", 
                 tuneGrid = tunegrid, 
                 trControl = ctrl)

#Shutdown cluster
stopCluster(cl)

```
The resulting cross-validation plot identify the best model.

```{r knn plot, echo=FALSE}
plot(knn_mdl)
```

After cross-validation training through the defined tune grid the best model found has the following tuning parameter:

```{r knn tuning, echo=FALSE}
knn_mdl$bestTune
```

After evaluating the accuracy of the model prediction to the test unseen data, the confusion matrix and the accuracy overall score is displayed. 
```{r knn evaluation, echo=FALSE}
y_hat <- predict(knn_mdl, newdata = fall_test_norm, type = 'raw')
confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$table
test_accuracy_knn <- confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$overall['Accuracy']
paste('model test accuracy:', round(test_accuracy_knn, 5), sep = '   ')
```
kNN model succeeded in classifying all 6 ADLs and it gets a good accuracy score considering that we have 6 class. Even Walking despite of class small numerosity is predicted with good accuracy.
The best model is obtained with a quite small k, indicating a complex model so more subject to overfitting.

```{r knn test result, include=FALSE}
test_results <- test_results %>% add_row(method = 'knn', accuracy = test_accuracy_knn)

```


## Random Forest
Random Forest algorithm builds multiple decision trees and merges them together to get a more accurate and stable prediction reducing variance and avoiding overfitting in respect of the single decision tree.
Random forest improves the predictive performance of decision tree through bagging, averaging models learned on multiple boostrapped samples from the original dataset,  and randomly selecting the predictors among which identify the one for partitioning data so that purest node are created at each split. 

The Random Forest model is implemented in R by 'randomForest' package [7].
Under the 'caret' framework it is possible to tune the following hyperparameter:
```{r rfParam, echo=FALSE}
modelLookup(model = 'rf')
```
mtry set the number of variables randomly sampled as candidates at each split.

The training process went through the following value for hyperparameters
```{r rf tune}
tunegrid <- data.frame(mtry= seq(1, 6, 1))
```

Note that setting mtry equal to the numbers of predictors is only bagging not exactly random forest (trees are not decorrelated because all trees use all the predictors) and it is expected to obtain a worse accuracy in cross validation.

```{r rf, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
if(!require(randomForest)) {
  install.packages("randomForest", repos = "http://cran.us.r-project.org")
  library(randomForest)
}

# setup parallel computing
cl <- makePSOCKcluster(parallel::detectCores()-1)
registerDoParallel(cl)

# train model
set.seed(525) 
rf_mdl <- train(ACTIVITY ~ .,
                 data = fall_train,
                 method = "rf", 
                 tuneGrid = tunegrid, 
                 trControl = ctrl)

#Shutdown cluster
stopCluster(cl)


```
The resulting cross-validation plot identify the best model.

```{r rf plot, echo=FALSE}
plot(rf_mdl)
```

After cross-validation training through the defined tune grid the best model found has the following tuning parameter:

```{r rf tuning, echo=FALSE}
rf_mdl$bestTune
```

After evaluating the accuracy of the model prediction to the test unseen data, the confusion matrix and the accuracy overall score is displayed. 
```{r rf evaluation, echo=FALSE}
y_hat <- predict(rf_mdl, newdata = fall_test, type = 'raw')
confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$table
test_accuracy_rf <- confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$overall['Accuracy']
paste('model test accuracy:', round(test_accuracy_rf, 5), sep = '   ')
```

Random Forest model succeeded in classifying all 6 ADLs and it gets a more than good accuracy score considering that we have 6 class. Even Walking despite of class small numerosity is predicted with good accuracy.
The best model is obtained with a small mtry.

```{r rf test result, include=FALSE}
test_results <- test_results %>% add_row(method = 'rf', accuracy = test_accuracy_rf)

```


# Results
The following table showed the results achieved for all models.

```{r accuracy, echo=FALSE}
test_results %>% knitr::kable()

```

Multinomial model tends to perform badly on the Fall Detection dataset because predictors are heavily correlated and multinomial model is a generalised linear model .

Better accuracy performance can be achieved with kNN a memory based algorithm but with a small number k of neighbors reveiling an intrinsic complexity and therefore a potential tendency to overfit.

Random Forest model predicts with a good accuracy all 6 ADLs because trees are able to better understand interactions among predictors.

## Interpretation
RandomForest is a black-box model with low interpretability even if considering the variable importance side result.
The interpretability issue related to machine learning black-box methods adoption has become relevant and a subject of discussion in the data science community. In this regard it is possible to mention the provoking titled paper "Why should I trust you? ..." [8] and the equally provoking book "Weapons of Math Destruction ..." [9].

So in order to provide explanation about the model result that could be validated by domain experts, a "ceteris paribus" analysis has been performed.
The concept behind this kind of analysis is very simple: one or more single predictions of interest for domain experts is taken under consideration and then  varying only one predictor at a time keeping all other variable unchanged (this is the meaning of latin word 'ceteris paribus': all else unchanged) what the model have learned can be assessed.

As an example the following prediction has been selected:
```{r  prediction selected, echo=FALSE}
knitr::kable(fall_test[15,])
```

The analysis for multiclass classification has been performed following the procedure documented at https://pbiecek.github.io/DALEX_docs/5-5-cetParLocalMulticlass.html#cetParLocalMulticlass for using "cerisParibus" package [10].

```{r ceteris paribus analysis, echo=FALSE}
if(!require(DALEX)) {
  install.packages("DALEX", repos = "http://cran.us.r-project.org")
  library(DALEX)
}
if(!require(ceterisParibus)) {
  install.packages("ceterisParibus", repos = "http://cran.us.r-project.org")
  library(ceterisParibus)
}

# prediction functions, one for each ADL
pred_Standing <- function(m, d) predict(m, d, type = 'prob')[,1]
pred_Walking <- function(m, d) predict(m, d, type = 'prob')[,2]
pred_Sitting <- function(m, d) predict(m, d, type = 'prob')[,3]
pred_Falling <- function(m, d) predict(m, d, type = 'prob')[,4]
pred_Cramps <- function(m, d) predict(m, d, type = 'prob')[,5]
pred_Running <- function(m, d) predict(m, d, type = 'prob')[,6]

# DALEX explainers, one for each ADL
explainer_Standing <- explain(model = rf_mdl$finalModel, 
                              data = fall_train[,2:7], 
                              y = fall_train$ACTIVITY == "Standing", 
                              predict_function = pred_Standing, 
                              label = "Standing")
explainer_Walking <- explain(model = rf_mdl$finalModel, 
                             data = fall_train[,2:7], 
                             y = fall_train$ACTIVITY == "Walking", 
                             predict_function = pred_Walking, 
                             label = "Walking")
explainer_Sitting <- explain(model = rf_mdl$finalModel, 
                             data = fall_train[,2:7], 
                              y = fall_train$ACTIVITY == "Sitting", 
                              predict_function = pred_Sitting, 
                              label = "Sitting")
explainer_Falling <- explain(model = rf_mdl$finalModel, 
                             data = fall_train[,2:7], 
                              y = fall_train$ACTIVITY == "Falling", 
                              predict_function = pred_Falling, 
                              label = "Falling")
explainer_Cramps <- explain(model = rf_mdl$finalModel, 
                            data = fall_train[,2:7], 
                            y = fall_train$ACTIVITY == "Cramps", 
                            predict_function = pred_Cramps, 
                            label = "Cramps")
explainer_Running <- explain(model = rf_mdl$finalModel, 
                             data = fall_train[,2:7], 
                              y = fall_train$ACTIVITY == "Running", 
                              predict_function = pred_Running, 
                              label = "Running")

# ceteris paribus profiles, one for each ADL
cp_Standing <- ceteris_paribus(explainer = explainer_Standing, 
                               observations = fall_test[15,],
                               y =fall_test$ACTIVITY[15]=="Standing")
cp_Walking <- ceteris_paribus(explainer_Walking, 
                              observations =  fall_test[15,],
                              y =fall_test$ACTIVITY[15]=="Walking")
cp_Sitting <- ceteris_paribus(explainer_Sitting, 
                              observations = fall_test[15,],
                              y =fall_test$ACTIVITY[15]=="Sitting")
cp_Falling <- ceteris_paribus(explainer_Falling, 
                              observations = fall_test[15,],
                              y =fall_test$ACTIVITY[15]=="Falling")
cp_Cramps <- ceteris_paribus(explainer_Cramps, 
                             observations = fall_test[15,],
                             y =fall_test$ACTIVITY[15]=="Cramps")
cp_Running <- ceteris_paribus(explainer_Running, 
                              observations = fall_test[15,],
                              y =fall_test$ACTIVITY[15]=="Running")

# plotting ceteris paribus profiles for the prediction of interest
plot(cp_Standing, cp_Walking, cp_Sitting, cp_Falling, cp_Cramps, cp_Running,
     alpha = 1, show_observations = FALSE, size_points = 4, color="_label_")

```

In order to understand how the resulting plot should be analysed it is useful to focus on only one variable plot: HR for example.

```{r varying HR plot, echo=FALSE}
# explanation of HR predictor for the prediction of interest
plot(cp_Standing, cp_Walking, cp_Sitting, cp_Falling, cp_Cramps, cp_Running,
     selected_variables = "HR",
     alpha = 1, show_observations = TRUE, size_points = 4, color="_label_")
```

At the selected prediction the probability of "Falling" is way higher than for other ADLs. There is a range varying only HR in which the prediction remains "Falling" even if predicted probabilities change.
Decreasing HR Heart Beat Rate does not change the prediction until below about 300 heart beat where the most probable ADLs become "Standing".
Also increasing HR till about 350 the model mantains the prediction at "Falling".
Without any domain expertise it is impossible to say if physiology theory can support the result learned by our model, but this is why generally Data Science Team includes domain experts together with Data Scientists.

# Conclusions
Going back to our research question, it is possible to state that ADLs can be predicted from basic health measures with an accuracy over 77%.
This means that the best model guesses the right ADL among 6 more than 3 times over 4. It is a remarkable result.

## Validity
Results can be considered valid because of this 3 main reasons:

- a training / validation / test study design has been followed consistently for all models 

- test and training/validation set contains thousands of observations

- all used machine learning techniques are consolidated

Furthermore ceteris paribus analysis presented could represent a feasable way to validate the model consistency with domain theory.

## Limitations
This project is a data science project in the context of supervised learning focused on the study of prediction.

Therefore the following 2 general limitations apply:
 
- from the results cannot be inferred anything about causation;

- the results validity depends on the quality of the data collected and contained in the dataset under study. Any sampling or measurement bias could be reflected in the results.

## Model improvements
Future research should look at evaluating different machine learning tecniques such as implementing a deep neural network with enough hidden layers in order to better learn the interactions between predictors.

Another possibility for increasing the accuracy of the prediction could be stacking: an ensemble method that builds a classification model at an upper level in regards of the studied models using prediction of the lower levels model as predictors for the upper level model.

In the contest of an actual scientific study other explanatory tool could be investigated and used so that domain experts can validate what the model have learned.

## Implications for my course of study
This project helped me in improving and consolidating:

- my understanding of the data science research methodology;

- the ability to communicate data science results in a reproducible report; 

- and the expert use of statistical computation tools.


# Reproducibilty
R script and rmarkdown files are available for review on public github repository: 

![](images/GitHub-Mark-16px.png) https://github.com/mdt-ds/PH125.9_cyo_fallDetection .


R script is intended to be reproducible so: 

- all package loading is checked for package installation.

- dataset is provided together with the code and loaded into R from my gitHub. 

- seed for random number generation has been set to guarantee reproducible results wherever it is needed

- number  of cores for parallel computation has not been hardcoded but retrieved with parallel::detectCores() function

- furthermore in order to facilitate reproducibility, HW and SW used for this project have been reported below.


## HW
The computation has been performed on my laptop:
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(benchmarkme)
library(parallel)
paste("Machine:     ", get_cpu()$model_name, sep = "")
paste("Num cores:   ", detectCores(logical = FALSE), sep = "")
paste("Num threads: ", detectCores(logical = TRUE), sep = "")
paste("RAM:         8GB")
```


## SW
The software stack is shown below launching sessionInfo() R function.
```{r repro, echo=FALSE, comment=''}
sessionInfo()
```

## Computation time
Sourcing the script containing all the analysis on my laptop configured as above took around 5 minutes to complete.

# Acknowledgments
I gratefully aknowledge the efforts of Professor Rafael Irizarry and all HarvardX  Course Staff for teaching this learning path towards a deeper understanding of Data Science. 


# References

[1] Özdemir, Ahmet Turan, and Billur Barshan. “Detecting Falls with Wearable Sensors  Using Machine Learning Techniques.” Sensors (Basel, Switzerland) 14.6 (2014): 10691–10708. PMC. Web. 23 Apr. 2017.

[2] Rafael Irizarry (2018). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. Chapters 30, 31, 32 and 33 https://rafalab.github.io/dsbook/

[3] Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre Williams, Chris Keefer,
Allan Engelhardt, Tony Cooper, Zachary Mayer, Brenton Kenkel, the R Core Team, Michael Benesty, Reynald Lescarbeau, Andrew Ziem, Luca Scrucca, Yuan Tang, Can Candan and Tyler Hunt. (2018). caret: Classification and Regression Training. R package version 6.0-81. https://CRAN.R-project.org/package=caret

[4] Microsoft Corporation and Steve Weston (2018). doParallel: Foreach Parallel Adaptor for the 'parallel' Package. R package version 1.0.14. https://CRAN.R-project.org/package=doParalle

[5] Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0
  
[6] David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel and Friedrich   Leisch (2019). e1071: Misc Functions of the Department of Statistics, Probability   Theory Group (Formerly: E1071), TU Wien. R package version 1.7-1.   https://CRAN.R-project.org/package=e1071
  
[7] A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R News 2(3), 18--22.

[8] Ribeiro, M.T., Singh, S., & Guestrin, C. (2016). "Why Should I Trust You?": Explaining the Predictions of Any Classifier. HLT-NAACL Demos.

[9] O'NEIL, C. (2016). Weapons of math destruction: how big data increases inequality and threatens democracy.

[10] Przemyslaw Biecek (2019). ceterisParibus: Ceteris Paribus Profiles. R package version 0.3.1. https://CRAN.R-project.org/package=ceterisParibus

***
\centering
![](images/cc-by-nc-sa.png)
