---
title: "PH125_9 Choose Your Own Capstone Project"
author: "Mario De Toma"
date: "February 28, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

## Objectives
*Motivation)*
This project has been conducted as part of the Data Science Professional Certification path provided by  HarvardX, an online learning initiative of Harvard University through edX.
In particular this is the second data science projet to submit for PH125.9x course denominated "Data Science: Capstone".
The name of the project, Choose Your Own, is due to the fact that the data set under analysis could be chosen by the learner from public available dataset. 
I choose the Fall Detection dataset from the curated list of datasets at the following link https://www.kaggle.com/annavictoria/ml-friendly-public-datasets?utm_medium=email&utm_source=intercom&utm_campaign=data+projects+onboarding as suggested by course staff. 

*Project objective)*
As per course project introduction the project aim is to apply machine learning techniques that go beyond standard linear regression. In particular the task of this project is  multi class classification i.e. where the outcome variable is categorical with more than 2 classes. Specifically the problem statement is related to predict the type of activity among 6 different activities of daily living (ADLs) on the basis of monitored medical measures obtained through sensors worn by elder people.


*Research question)*
The research question can be stated as:
is it possible to predict 6 activities of daily living (ADLs) including Standing , Walking, Sitting, Falling,  Cramps, Running from few predictors monitoring health status?

This project is not intended to solve the problem of Fall Detection which could be settled by binarysing the ADL information as Falling vs all the remaining activities.

Furthermore note that no causal inference claim can be raised after this study that focus only on supervised learning.

*Dataset)*
Fall detection data set of Chinese hospitals of old age patients [1] is hosted by kaggle. 
It reports 16382 observations containing the ADL label and related 6 predictors.

## Background
Starting point for conducting this study is the supervised leaning process as decribed by Professor Rafael Irizarry in the PH125_8 edX course on Machine Learning and in his book: Introduction to Data Science [2].
In particular study is conducted using the 'caret' R package framework for machine learning [3].

## Overview and outline
The study demonstrates that ADL can be predicted and that reachable accuracy of prediction depends on the model chosen.

This report is articulated in the following sections:

- *Methods*: where the dataset is explored in order to find some insight, then the design of the study is explained and different models are proposed. Finally the modeling will be described in details.

- *Results*: showing actual results achieved and models evaluation compared

- *Conclusions*: summarizing achievement, discussing the project and indicate potential  model improvement

- *Reproducibility*: providing information related to the reproducibility of the analysis including computation considerations, HW and SW stack used.

# Methods
In order to answer the research question posed in the introcuction section, the dataset Fall Detection has been analyzed and then the machine learning experiments consucted using different machine learning tecniques adequate for the multi class classificaion task.

```{r libraries, include=FALSE }
if(!require(tidyverse)) {
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
  library(tidyverse)
}
if(!require(caret)) {
  install.packages("caret", repos = "http://cran.us.r-project.org")
  library(caret)
}
```

## Exploratory Data Analysis
Fall detection dataset has been downloaded from kaggle, put on my github and then loaded into R and partitioned such that 70% of the observations belong to the training set and 30% the test set. The caret::createDataPartition function has been used in order to mantain the class distribution between train and test set.
```{r load data, message=FALSE, include=FALSE}
data_file_path <- 'https://raw.githubusercontent.com/mdt-ds/PH125.9_cyo_fallDetection/master/falldeteciton.csv'
fall_detection <- read_csv(file = data_file_path)
set.seed(525)
train_idx <- createDataPartition(y = fall_detection$ACTIVITY, times = 1, 
                                 p = 0.7, list = FALSE)
fall_train <- fall_detection %>% slice(train_idx)
fall_test <- fall_detection %>% slice(-train_idx)
rm(data_file_path, train_idx)
```

Fall Detection dataset contains the following valriables:
```{r Fall Detection structure, echo=FALSE}
glimpse(fall_train)
```
The class label ACTIVITY is a numeric variable. It had been converted to a factor with following levels: 0- Standing 1- Walking 2- Sitting 3- Falling 4- Cramps 5- Running
```{r label}
fall_train <- fall_train %>% mutate(ACTIVITY = factor(ACTIVITY, 
                             labels = c('Standing', 'Walking', 'Sitting', 
                                        'Falling', 'Cramps', 'Running')))
fall_test <- fall_test %>% mutate(ACTIVITY = factor(ACTIVITY, 
                              labels = c('Standing', 'Walking', 'Sitting', 
                                        'Falling', 'Cramps', 'Running')))

```

### ADL Distribution
The ADL classes are not evenly distributed. In particular Walking ADL has few observations. This could make our multi class classification task harder.

```{r ADLs Distribution, echo=FALSE}
ggplot(fall_train, aes(ACTIVITY)) +
  geom_bar(aes(fill = ACTIVITY)) +
  labs(title = 'ADLs Distribution')
```

### Predictors discriminative power
Predictors for ADL in Fall Detection dataset are:

- TIME monitoring time

- SL sugar level

- EEG monitoring rate

- BP blood pressure

- HR Heart beat rate

- CIRCLUATION Blood circulation

In order to check single predictor capacity to discriminate among classes, for each predictor the boxplot by class has been drawn.

```{r boxplot by ADL, echo=FALSE}
timeBP <- ggplot(fall_train, aes(ACTIVITY, TIME)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('monitoring time') +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 7))

slBP <- ggplot(fall_train, aes(ACTIVITY, SL)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('sugar level') +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 7))

eegBP <- ggplot(fall_train, aes(ACTIVITY, EEG)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('EEG monitoring rate') +
  theme(legend.position = 'none',
        axis.text.x = element_text(size = 7))

bloodBP <- ggplot(fall_train, aes(ACTIVITY, BP)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('blood pressure') +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 7))

hrBP <- ggplot(fall_train, aes(ACTIVITY, HR)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('heart beat rate') +
  theme(legend.position = 'none', 
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 7))

circBP <- ggplot(fall_train, aes(ACTIVITY, CIRCLUATION)) +
  geom_boxplot(aes(fill = ACTIVITY)) +
  ylab('blood circulation') +
  theme(legend.position = 'none',
        axis.text.x = element_text(size = 7))

if(!require(gridExtra)) {
  install.packages("gridExtra", repos = "http://cran.us.r-project.org")
  library(gridExtra)
}
grid.arrange(timeBP, slBP, bloodBP, hrBP, circBP, eegBP, 
              nrow = 3, ncol = 2, newpage = TRUE)

```
The discriminative power of single predictors does not seem to be enough to classify ADL. 

### Predictors correlation
Further element of analysis is related to high level of correlation between predictors.

```{r corr, echo=FALSE}
if(!require(GGally)) {
  install.packages("GGally", repos = "http://cran.us.r-project.org")
  library(GGally)
}
ggpairs(fall_train %>% select(-ACTIVITY), axisLabels = 'none')
```
In particular SL (sugar level) and CIRCLUATION (blood circulation) with a correlation of 0.977 and TIME (monitoring time) and HR (heart beat rate) with a correlation of 0.974 are almost colinear. Also HR and CIRCULATION (0.904), HR with SL (0.857) and TIME with CIRCULATION (0.877) correlations are really high.

### Feature importance
The feature importance has been therefore investigated through the random forest algorithm which provide as a side outcome the importance of a feature in discriminating one class from the other. In other words, for random forest algorithm is easy to compute how much each variable is contributing to the classification decision.
```{r varImp, echo=FALSE}
if(!require(randomForest)) {
  install.packages("randomForest", repos = "http://cran.us.r-project.org")
  library(randomForest)
}

rf_mdl <- randomForest(ACTIVITY ~ ., data = fall_train)
varImpPlot(rf_mdl)
```
From the plot, SL is the most important predictor followed by EEG but all 6 are are of great help in the classification attempt.

## Proposed models
In order to accomplish the multi class classification task the followin models has been tried:

- multinomial

- k Nearest Neighbors

- random forest

All the models are natural choice for multi class classification.

## Study design
The study will be conducted using the training set, fall_train, for training and tuning of hyperparameters through cross validation while the final accuracy will be evaluated on the test set, fall_test.

The test dataset is not used in any former phase of the study and therefore it can simulate new data allowing to evaluate the capacity to generalize of the model.

Accuracy metric has been used to evaluate different models. For each model the confusion matrix will be produced in order to evaluate which class is harder to identify.

Cross validation has been configured in caret machine learning framework with 5 folds (80% for training, 20% for validation)

```{r train control}
ctrl <- trainControl(method = 'cv', number = 5, p = .8)
```

Cross validation lead to long computation times because in the defined study design the model has to be trained and validated 5 times.
Therefore cross validation computation has been parallelized making use of doParallel package and of the multithread architecture of the laptop used for this project.
```{r parallel, message=FALSE, warning=FALSE, include=FALSE}
if(!require(doParallel)) {
  install.packages("doParallel", repos = "http://cran.us.r-project.org")
  library(doParallel)
}
```


## Multinomial Logistic Regression
Multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems. The probability to belong to a particular class given the predictors is formulated as:

$P(y=k|x^{(i)},\theta)=\frac{\exp(\theta^{(k)T}x^{(i)})}{\sum_{j=1}^k\exp(\theta^{(j)T}x^{(i)})}$

It is called also softmax regression.

The multinomial model is implemented in r via neural networks by 'nnet' package.
Under the 'caret' it is possible to tune the following parameter:
```{r multinomParam, echo=FALSE}
modelLookup(model = 'multinom')
```
The parameter Weight Decay is specific to neural networks and it helps the optimization process avoiding over-fitting.
The training process will go through the following value for hyperparameters
```{r multinom tune}
tunegrid <- data.frame(decay = seq(from = 0, to = 10, by = 1))
```


```{r multim, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
if(!require(nnet)) {
  install.packages("nnet", repos = "http://cran.us.r-project.org")
  library(nnet)
}

# setup parallel computing
cl <- makePSOCKcluster(parallel::detectCores()-1)
registerDoParallel(cl)

# train model
set.seed(525) 
multinom_mdl <- train(ACTIVITY ~ .,
                 data = fall_train,
                 method = "multinom", 
                 tuneGrid = tunegrid, 
                 trControl = ctrl)

#Shutdown cluster
stopCluster(cl)


```
The resulting cross-validation plot identify the best model.

```{r multinom plot, echo=FALSE}
plot(multinom_mdl)
```

After cross-validation training through the defined tune grid the best model found has the following parameter:

```{r multinom tuning, echo=FALSE}
multinom_mdl$bestTune
```

After evaluating the accuracy of the model prediction to the test unseen data, the confusion matrix and the accuracy overall score is displayed. 
```{r multinom evaluation, echo=FALSE}
y_hat <- predict(multinom_mdl, newdata = fall_test, type = 'raw')
confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$table
test_accuracy_multinom <- confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$overall['Accuracy']
paste('model test accuracy:', round(test_accuracy_multinom, 5), sep = '   ')
```
As per this results, the multinomial model can be discarded.
In some sense the bad accuracy results were expected since predictors are correlated and interconnected while multinomial as a type of general linear model has difficulty in getting the interactions.
Furtermore we can see from the confusion matrix that Walking ADL is never predicted probably because it has few occurrence in the dataset.
Also Running ADL is never predicted.
```{r multinom test result, include=FALSE}
test_results <- tibble(method = 'multinom', accuracy = test_accuracy_multinom)

```


## k Nearest Neighbour
k Nearest Neighbors is a non-parametric classification method that make use of distance (or similarity) measures. In particular for numerical predictors the euclidean distance is used. Euclidean distance is the length of the segment connecting 2 data points in the predictor space and it is defined as:

$d(\overrightarrow{x}^{(i)},\overrightarrow{x}^{(j)}) = \sqrt{(x_1^{(i)}-x_1^{(j)})^2+(x_2^{(i)}-x_2^{(j)})^2+...+(x_p^{(i)}-x_p^{(j)})^2}$

The kNN algorithm stored all the data and classify new data points in relation of majority of k nearest (as per euclidean distance) points class.

The k Nearest Neighbors model is implemented in r by 'e1071' package.
Under the 'caret' machine learning famework it is possible to tune the following parameter:
```{r knnParam, echo=FALSE}
modelLookup(model = 'knn')
```
As parameter k increase the decision boundary get more smooth. It can be thought as a mean of regurarizaion.
The training process will go through the following value for hyperparameters
```{r knn tune}
tunegrid <- data.frame(k= seq(1, 30, 1))
```
Given that kNN algorithm is based on distance/similarity measures, data needs to be scaled (by dividing by respective standard deviation) and centered (by subctracing the mean) before traning  in order to avoid that predictors with largest numerical range mask the effect of other predictors. 
```{r scale data, echo=FALSE}
fall_train_norm <- predict(preProcess(fall_train, method = c("center", "scale")),
                            newdata = fall_train)
fall_test_norm <- predict(preProcess(fall_train, method = c("center", "scale")),
                          newdata = fall_test)
```


```{r knn, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
if(!require(e1071)) {
  install.packages("e1071", repos = "http://cran.us.r-project.org")
  library(e1071)
}

# setup parallel computing
cl <- makePSOCKcluster(parallel::detectCores()-1)
registerDoParallel(cl)

# train model
set.seed(525) 
knn_mdl <- train(ACTIVITY ~ .,
                 data = fall_train_norm,
                 method = "knn", 
                 tuneGrid = tunegrid, 
                 trControl = ctrl)

#Shutdown cluster
stopCluster(cl)

```
The resulting cross-validation plot identify the best model.

```{r knn plot, echo=FALSE}
plot(knn_mdl)
```

After cross-validation training through the defined tune grid the best model found has the following parameter:

```{r knn tuning, echo=FALSE}
knn_mdl$bestTune
```

After evaluating the accuracy of the model prediction to the test unseen data, the confusion matrix and the accuracy overall score is displayed. 
```{r knn evaluation, echo=FALSE}
y_hat <- predict(knn_mdl, newdata = fall_test_norm, type = 'raw')
confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$table
test_accuracy_knn <- confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$overall['Accuracy']
paste('model test accuracy:', round(test_accuracy_knn, 5), sep = '   ')
```
kNN model succeed in classifying all 6 ADLs and it gets a good accuracy score considering that we have 6 class. Even Walking despite of class small numerosity is predicted with good accuracy.
The best model is obtained with a quite small k, a complex model, as expected because of the interactions among predictors.

```{r knn test result, include=FALSE}
test_results <- test_results %>% add_row(method = 'knn', accuracy = test_accuracy_knn)

```


## Random Forest
Random Forest algorithm builds multiple decision trees and merges them together to get a more accurate and stable prediction reducing variance and avoiding overfitting in respect of the single decision tree.
Random forest improves the predictive performance of decision tree through bagging, averaging models learned on multiple boostrapped samples from the original dataset,  and randomly selecting the predictors among which identify the one for partitioning data so that purest node are created at each split. 

The Random Forest model is implemented in r by 'randomForest' package.
Under the 'caret' it is possible to tune the following parameter:
```{r rfParam, echo=FALSE}
modelLookup(model = 'rf')
```
that set the number of variables randomly sampled as candidates at each split.

The training process will go through the following value for hyperparameters
```{r rf tune}
tunegrid <- data.frame(mtry= seq(1, 5, 1))
```

```{r rf, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
if(!require(randomForest)) {
  install.packages("randomForest", repos = "http://cran.us.r-project.org")
  library(randomForest)
}

# setup parallel computing
cl <- makePSOCKcluster(parallel::detectCores()-1)
registerDoParallel(cl)

# train model
set.seed(525) 
rf_mdl <- train(ACTIVITY ~ .,
                 data = fall_train,
                 method = "rf", 
                 tuneGrid = tunegrid, 
                 trControl = ctrl)

#Shutdown cluster
stopCluster(cl)


```
The resulting cross-validation plot identify the best model.

```{r rf plot, echo=FALSE}
plot(rf_mdl)
```

After cross-validation training through the defined tune grid the best model found has the following parameter:

```{r rf tuning, echo=FALSE}
rf_mdl$bestTune
```

After evaluating the accuracy of the model prediction to the test unseen data, the confusion matrix and the accuracy overall score is displayed. 
```{r rf evaluation, echo=FALSE}
y_hat <- predict(rf_mdl, newdata = fall_test, type = 'raw')
confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$table
test_accuracy_rf <- confusionMatrix(data = y_hat, reference = fall_test$ACTIVITY)$overall['Accuracy']
paste('model test accuracy:', round(test_accuracy_rf, 5), sep = '   ')
```

Random Forest model succeed in classifying all 6 ADLs and it gets a more than good accuracy score considering that we have 6 class. Even Walking despite of class small numerosity is predicted with good accuracy.
The best model is obtained with a small mtry.

```{r rf test result, include=FALSE}
test_results <- test_results %>% add_row(method = 'rf', accuracy = test_accuracy_rf)

```


# Results
The following table showed the results achieved for all models.

```{r accuracy, echo=FALSE}
test_results %>% knitr::kable()

```

Multinomial model tends to perform badly on the Fall Detection dataset because predictors are heavily correlated.
Better accuracy performance can be achieved with kNN a memory based algorithm but with a small number k of neighbors reveiling an intrinsic complexity.
Random Forest model predicts with a good accuracy all 6 ADLs because trees understand interactions between predictors.

# Conclusions
Going back to our research question, it is possible to state that ADLs can be predicted from basic health measuree with an accuracy over 77%.
This means that the best model guesses the right ADL among 6 more than 3 times over 4. It is a remarkable result.

## Validity
Results can be considered valid because of this 3 main reasons:

- a consistent training / validation / test study design has been followed consistently for all models 

- test and training/validation set contains thousands of observaions

- all used machine learning techniques are consolidated

## Limitations
This project is a data science project in the context of supervised learning focused on the study of prediction.

Therefore the following 2 general limitations apply:
 
- from the results cannot be inferred anything about causation;

- the results validity depends on the accuracy of the data collected and contained in the dataset under study. Any sampling or measurement bias could be reflected in the results.

## Model improvements
Future research should look at evaluating different machine learning tecniques such as implementing a deep neural network with enough hidden layers for getting all the interactions between predictors.
Another possibility for increasing the accuracy of the prediction could be stacking: an ensemble method that builds a classification model at an upper level in regards of the studied models using prediction of the lower levels model as predictors for the upper level model.

## Implications
This project helped me in consolidating

- my understanding of the data science research methodology;

- the ability to communicate data science results in a reproducible report; 

- and the expert use of statistical computation tools.


# Reproducibilty
R script and rmarkdown file are available for review on public github repository: 

![](images/GitHub-Mark-16px.png) https://github.com/mdt-ds/PH125.9_cyo_fallDetection .


R scripts are intended to be reproducible. 

- All package loading is checked for package installation.

- Directoty are all indicated in relative fashion. 

- Seed for random number generation has been set to guarantee reproducible results wherever it is needed

- Furthermore in order to facilitate reproducibility, HW and SW used for this project have been reported below.


## HW
The computation has been performed on my laptop:
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(benchmarkme)
library(parallel)
paste("Machine:     ", get_cpu()$model_name, sep = "")
paste("Num cores:   ", detectCores(logical = FALSE), sep = "")
paste("Num threads: ", detectCores(logical = TRUE), sep = "")
paste("RAM:         8GB")
```


## SW
The software stack is shown below launching sessionInfo() R function.
```{r repro, echo=FALSE, comment=''}
sessionInfo()
```

## Computation time
Sourcing the script containing all the analysis on my laptop configured as above took about 138 seconds to complete.

# Acknowledgments
I gratefully aknowledge the efforts of Professor Rafael Irizarry and all HarvardX  Course staff for teaching this learning path towards a deeper understanding of Data Science. 


# References

[1] Özdemir, Ahmet Turan, and Billur Barshan. “Detecting Falls with Wearable Sensors Using Machine Learning Techniques.” Sensors (Basel, Switzerland) 14.6 (2014): 10691–10708. PMC. Web. 23 Apr. 2017.

[2] Rafael Irizarry (2018). Introduction to Data Science. Data Analysis and Prediction Algorithms with R. Chapters 71, 72 and 73 https://rafalab.github.io/dsbook/

[3] Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre Williams, Chris Keefer,
Allan Engelhardt, Tony Cooper, Zachary Mayer, Brenton Kenkel, the R Core Team, Michael Benesty, Reynald Lescarbeau, Andrew Ziem, Luca Scrucca, Yuan Tang, Can Candan and Tyler Hunt. (2018). caret: Classification and Regression Training. R package version 6.0-81. https://CRAN.R-project.org/package=caret

***
\centering
![](images/cc-by-nc-sa.png)
